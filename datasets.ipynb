{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load datasets voteview.com\n",
    "\n",
    "# dataset containing information on U.S. legislators\n",
    "members = pd.read_csv('HSall_members.csv', sep=',')\n",
    "# dataset containing information on roll calls\n",
    "roll_calls = pd.read_csv('HSall_rollcalls.csv', sep=',')\n",
    "# dataset containing information on votes taken by legislators on roll calls\n",
    "votes = pd.read_csv('HSall_votes.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove votes from before 93th congress\n",
    "votes = votes[votes['congress'] > 92]\n",
    "\n",
    "# remove votes from Senate\n",
    "votes = votes[votes['chamber'] == 'House']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join votes from 93th congress and members together\n",
    "votes_members = pd.merge(votes, members, on=['icpsr', 'congress', 'chamber'], how='left')\n",
    "votes_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add official name of party to dataset\n",
    "party_name = {100: 'Democratic Party', 200: 'Republican Party', 112: 'Conservative Party', 328: 'Independent', 370: 'Progressive Party',\n",
    "              537: 'Farmer-Labor Party', 331: 'Independent Republican', 380: 'Socialist Party', 329: 'Independent Democrat', \n",
    "              522: 'American Labor Party', 340: 'Populist Party', 347: 'Prohibitionist Party', 356: 'Union Labor Party', \n",
    "              213: 'Progressive Republican Party', 402: 'Liberal Party', 354: 'Silver Republican Party', 523: 'American Labor Party (La Guardia)'} \n",
    "\n",
    "votes_members['party'] = votes_members['party_code'].map(party_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add codes of casting to dataset\n",
    "cast_codes = {0: 'NA member', 1: 'Yea', 2: 'Paired Yea', 3: 'Announced Yea', 4: 'Announced Nay', 5: 'Paired Nay', 6: 'Nay', \n",
    "              7: 'Present', 8: 'Present', 9: 'Abstention'}\n",
    "\n",
    "votes_members['casting'] = votes_members['cast_code'].map(cast_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add state full names to dataset\n",
    "states = {'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut',\n",
    "          'DE': 'Delaware', 'DC': 'District of Columbia', 'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', \n",
    "          'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', \n",
    "          'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', \n",
    "          'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', \n",
    "          'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', \n",
    "          'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', \n",
    "          'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming', 'AS': 'American Samoa', 'MP': 'Northern Mariana Islands', 'GU': 'Guam',\n",
    "          'PR': 'Puerto Rico', 'VI': 'Virgin Islands'}\n",
    "\n",
    "votes_members['state'] = votes_members['state_abbrev'].map(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns of dataframe to make it better organized\n",
    "order = ['bioname', 'party', 'state', 'state_abbrev', 'district_code', 'casting', 'congress', 'chamber', 'rollnumber', \n",
    "         'icpsr', 'state_icpsr', 'occupancy', 'last_means', 'born', 'died', 'bioguide_id', 'party_code',  'cast_code', 'prob', \n",
    "         'nominate_dim1', 'nominate_dim2', 'nominate_log_likelihood', 'nominate_geo_mean_probability', 'nominate_number_of_votes', \n",
    "         'nominate_number_of_errors', 'conditional', 'nokken_poole_dim1', 'nokken_poole_dim2'] \n",
    "\n",
    "votes_members = votes_members[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe containing all information on member and votes. rows are member's votes.\n",
    "with open(\"votes_members_full.tsv\", \"w+\") as f:\n",
    "    votes_members.to_csv(f, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in member votes dataset\n",
    "df_members = pd.read_csv('votes_members_full.tsv', sep='\\t')\n",
    "\n",
    "# only keep important columns\n",
    "cols = ['bioname','congress', 'party', 'chamber', 'icpsr', 'state', 'district_code', 'bioguide_id']\n",
    "df_members = df_members[cols]\n",
    "\n",
    "# remove duplicates to keep only members per congress\n",
    "df_members = df_members.drop_duplicates()\n",
    "\n",
    "# create id based on name and party combination\n",
    "df_members['nameparty_id'] = 'np_' + (df_members.groupby(['bioname', 'party']).ngroup() + 1).astype(str).str[:-2]\n",
    "\n",
    "# create id based on name \n",
    "df_members['name_id'] = 'n_' + (df_members.groupby(['bioname']).ngroup() + 1).astype(str).str[:-2]\n",
    "\n",
    "# create id based on name, party, and congress\n",
    "df_members['member_id'] = 'm_' + (df_members.groupby(['bioname', 'party', 'congress']).ngroup() + 1).astype(str).str[:-2]\n",
    "\n",
    "# save final members dataset\n",
    "with open(\"df_members_fv.tsv\", \"w+\") as f:\n",
    "    df_members.to_csv(f, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add year column to roll call dataframe\n",
    "roll_calls['year'] = roll_calls['date'].astype(str).str[:4].astype(int)\n",
    "\n",
    "# remove roll calls from before 93th congress\n",
    "df_bills = roll_calls[roll_calls['congress'] > 92]\n",
    "\n",
    "# remove roll calls from the Senate\n",
    "df_bills = df_bills[df_bills['chamber'] == 'House']\n",
    "\n",
    "# take important columns from dataframe and create subdataframe that is only about bills \n",
    "bill_order = ['congress', 'chamber', 'rollnumber', 'clerk_rollnumber', 'bill_number', 'year', 'vote_result', 'yea_count', 'nay_count', \n",
    "              'vote_desc', 'vote_question', 'dtl_desc']\n",
    "              \n",
    "df_bills = df_bills[bill_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: bill numbers from one dataset contained false letter combinations, leading to trouble merging with new dataset \n",
    "\n",
    "# lowercase bill number\n",
    "df_bills['bill_number'] = df_bills['bill_number'].str.lower()\n",
    "\n",
    "# divide bill number into number and letters\n",
    "df_bills['bill'] = df_bills['bill_number'].str.replace(r'\\d+', '', regex=True)\n",
    "df_bills['number'] = df_bills['bill_number'].str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "# replace wrong letter combinations by correct combinations\n",
    "replace_dict = {\n",
    "    'hr': 'hr',\n",
    "    'hres': 'hres',\n",
    "    's': 's',\n",
    "    'hconres': 'hconres',\n",
    "    'hjres': 'hjres',\n",
    "    'hre': 'hres',\n",
    "    'hjr': 'hjres',\n",
    "    'hcr': 'hconres',\n",
    "    'sjres': 'sjres',\n",
    "    'hconr': 'hconres',\n",
    "    'sconres': 'sconres',\n",
    "    'sjr': 'sjres',\n",
    "    'scr': 'sconres',\n",
    "    'hcon': 'hconres',\n",
    "    'hjre': 'hjres',\n",
    "    'hcre': 'hconres',\n",
    "    'sjre': 'sjres',\n",
    "    'hcres': 'hconres',\n",
    "    'sconr': 'sconres',\n",
    "    'scres': 'sconres',\n",
    "    'scon': 'sconres',\n",
    "    'scre': 'sconres',\n",
    "    'sres': 'sres',\n",
    "    'h': 'hr',\n",
    "    'hj': 'hjres',\n",
    "    'hhr': 'hr'\n",
    "}\n",
    "df_bills['bill_1'] = df_bills['bill'].map(replace_dict)\n",
    "\n",
    "# create new correct bill numbers\n",
    "df_bills['bill_number_1'] = df_bills['bill_1'] + df_bills['number']\n",
    "\n",
    "# drop old and unneeded columns\n",
    "df_bills.drop(columns=['bill', 'number', 'bill_1', 'bill_number'], inplace=True)\n",
    "\n",
    "# change column name and add column with bill type\n",
    "df_bills['bill'] = df_bills['bill_number_1'].str.replace(r'\\d+', '', regex=True)\n",
    "df_bills.rename(columns={'bill_number_1': 'bill_number'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding extra bill information\n",
    "Topic modelling on the information present in the voteview.org dataset did not perform well. \n",
    "\n",
    "We therefore decided to include information on the bill from ProPublica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape summary information from json and xml files\n",
    "def get_summary(congress, bill, bill_number):\n",
    "    try:\n",
    "        # find json file per congress, bill type, and bill number\n",
    "        file_path = f\"congress/{congress}/{bill}/{bill_number}/data.json\"\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # find summary in data and get text\n",
    "                summary = data['summary']['text']\n",
    "            return summary\n",
    "        else:\n",
    "            # if no json file, find xml file per congress, bill type, and bill number\n",
    "            xml_file_path = f\"congress/{congress}/{bill}/{bill_number}/fdsys_billstatus.xml\"\n",
    "            if os.path.exists(xml_file_path):\n",
    "                tree = ET.parse(xml_file_path)\n",
    "                root = tree.getroot()\n",
    "                # find summaries in root\n",
    "                summaries = root.find('.//summaries')\n",
    "                if summaries is not None:\n",
    "                    # summaries often contain multiple (similar) texts\n",
    "                    text_list = []\n",
    "                    for summary in summaries.findall('.//summary'):\n",
    "                        # get text from each summary and append into list\n",
    "                        text_element = summary.find('.//text')\n",
    "                        if text_element is not None:\n",
    "                            text = text_element.text.strip()\n",
    "                            text_list.append(text)\n",
    "                        else:\n",
    "                            return None\n",
    "                    # remove duplicate summaries\n",
    "                    text_list = list(set(text_list))\n",
    "                    filtered_list = [item for item in text_list if item != '']\n",
    "                    # return summary as string \n",
    "                    result_string = ', '.join(filtered_list)\n",
    "                    return result_string\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "# add summary to new column based on congress, bill type, and bill number\n",
    "df_bills['summary'] = df_bills.apply(lambda row: get_summary(row['congress'], row['bill'], row['bill_number']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill rows with no summary with empty string\n",
    "df_bills['summary'] = df_bills['summary'].fillna('')\n",
    "\n",
    "# Problem: from xml files the html text was also copied\n",
    "# remove html text\n",
    "df_bills['summary'] = df_bills['summary'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "# save new dataframe with all rows containing summary information\n",
    "with open(\"df_bills.tsv\", \"w+\") as f:\n",
    "    df_bills.to_csv(f, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding topic information \n",
    "To be able to test if topic modelling on summary works correctly, we included policy information from ProPublica to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get main policy area of bill (only from xml files)\n",
    "def get_policy1(congress, bill, bill_number):\n",
    "    try:\n",
    "        # find xml file per congress, bill type, and bill number\n",
    "        xml_file_path = f\"congress/{congress}/{bill}/{bill_number}/fdsys_billstatus.xml\"\n",
    "        if os.path.exists(xml_file_path):\n",
    "            tree = ET.parse(xml_file_path)\n",
    "            root = tree.getroot()\n",
    "            # find policy in root\n",
    "            policy = root.find('.//policyArea')\n",
    "            if policy is not None:\n",
    "                # policy can contain multiple (similar) texts\n",
    "                name_list = []\n",
    "                # find name of policy\n",
    "                name = policy.find('name')\n",
    "                if name is not None:\n",
    "                    # take text from name and put in list\n",
    "                    name = name.text.strip()\n",
    "                    name_list.append(name)\n",
    "                else:\n",
    "                    return None\n",
    "                # remove duplicate policies\n",
    "                name_list = list(set(name_list))\n",
    "                filtered_list = [item for item in name_list if item != '']\n",
    "                # return policy as string\n",
    "                result_string = ', '.join(filtered_list)\n",
    "                return result_string\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# function to get main policy area of bill (only for json files)\n",
    "def get_policy2(congress, bill, bill_number):\n",
    "    try:\n",
    "        # find json file per congress, bill type, and bill number\n",
    "        file_path = f\"congress/{congress}/{bill}/{bill_number}/data.json\"\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # find top subject in data (equal to policy)\n",
    "                policy = data['subjects_top_term']\n",
    "            return policy\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "        \n",
    "# add main policy area to new column based on congress, bill type, and bill number\n",
    "df_bills['policy_1'] = df_bills.apply(lambda row: get_policy1(row['congress'], row['bill'], row['bill_number']), axis=1)\n",
    "\n",
    "# add main policy area to new column based on congress, bill type, and bill number\n",
    "df_bills['policy_2'] = df_bills.apply(lambda row: get_policy2(row['congress'], row['bill'], row['bill_number']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join policy columns together. if both have a different policy, then combine the two. if both have the same, then take only one of them.\n",
    "df_bills['policy'] = np.where((~df_bills['policy_1'].isnull()) & (~df_bills['policy_2'].isnull()) & (df_bills['policy_1'] != df_bills['policy_2']),\n",
    "                                 df_bills['policy_1'].astype(str) + ', ' + df_bills['policy_2'].astype(str),\n",
    "                                 df_bills['policy_1'].combine_first(df_bills['policy_2']))\n",
    "\n",
    "# drop old policy columns\n",
    "df_bills = df_bills.drop(columns=['policy_1', 'policy_2'])\n",
    "\n",
    "# add bill id\n",
    "df_bills['bill_id'] = ['b_' + str(i) for i in range(1, len(df_bills) + 1)]\n",
    "\n",
    "# find bills without summary\n",
    "no_sum = df_bills[df_bills['summary'] == '']\n",
    "\n",
    "# find indice of those bills\n",
    "no_sum_indices = no_sum.index\n",
    "\n",
    "# remove bills from dataset that have no summary\n",
    "df_bills = df_bills.drop(no_sum_indices)\n",
    "\n",
    "# save final bills dataset\n",
    "with open(\"df_bills_fv.tsv\", \"w+\") as f:\n",
    "    df.to_csv(f, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in member votes dataset\n",
    "df_member_votes = pd.read_csv('votes_members_full.tsv', sep='\\t')\n",
    "\n",
    "# remove votes without bill number\n",
    "df_member_votes = df_member_votes.dropna(subset=['bill_number'])\n",
    "\n",
    "# drop not important columns\n",
    "df_member_votes.drop(columns=['clerk_rollnumber'], inplace=True)\n",
    "df_member_votes.drop(columns=['district_code'], inplace=True)\n",
    "\n",
    "# drop duplicate votes\n",
    "df_member_votes = df_member_votes.drop_duplicates()\n",
    "\n",
    "# create three dataset from members dataset, each containing own id\n",
    "m_col = df_members[['bioname', 'party', 'congress', 'member_id']]\n",
    "n_col = df_members[['bioname', 'name_id']]\n",
    "np_col = df_members[['bioname', 'party', 'nameparty_id']]\n",
    "\n",
    "# drop duplicates, thereby only keeping rows with unique id\n",
    "n_col = n_col.drop_duplicates()\n",
    "m_col = m_col.drop_duplicates()\n",
    "np_col = np_col.drop_duplicates()\n",
    "\n",
    "# add name id to votes dataset\n",
    "df_member_votes = pd.merge(df_member_votes, n_col, on=['bioname'], how='left')\n",
    "df_member_votes = df_member_votes.drop_duplicates()\n",
    "\n",
    "# add name party id to votes dataset\n",
    "df_member_votes = pd.merge(df_member_votes, np_col, on=['bioname', 'party'], how='left')\n",
    "df_member_votes = df_member_votes.drop_duplicates()\n",
    "\n",
    "# add member id to votes dataset\n",
    "df_member_votes = pd.merge(df_member_votes, m_col, on=['bioname', 'party', 'congress'], how='left')\n",
    "df_member_votes = df_member_votes.drop_duplicates()\n",
    "\n",
    "# create dataset from bills dataset only keeping id information\n",
    "b_col = df_bills[['congress', 'bill_number', 'rollnumber', 'bill_id']]\n",
    "\n",
    "# drop duplicates, thereby only keeping rows with unique bill id\n",
    "b_col = b_col.drop_duplicates()\n",
    "\n",
    "# add bill id to votes dataset\n",
    "df_member_votes = pd.merge(df_member_votes, b_col, on=['congress', 'bill_number', 'rollnumber'], how='left')\n",
    "\n",
    "# save final votes dataset\n",
    "with open(\"df_member_votes_fv.tsv\", \"w+\") as f:\n",
    "    df_member_votes.to_csv(f, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After LDA Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset bills with topic\n",
    "df_topic = pd.read_csv('df_bills_fv_with_topics_3.tsv', sep='\\t')\n",
    "df_mv = pd.read_csv('df_member_votes_fv.tsv', sep='\\t')\n",
    "\n",
    "# only keep relevant columns from topic dataset\n",
    "cols = ['bill_id', 'topic']\n",
    "df_topic = df_topic[cols]\n",
    "\n",
    "# join votes and topic datasets together\n",
    "df_mv = pd.merge(df_topic, df_bt, on=['bill_id'], how='left')\n",
    "\n",
    "# save dataframe with votes and topics, which can be used for NOMINATE and CA methods\n",
    "with open(\"df_member_votes_fv_with_topics_3.tsv\", \"w+\") as f:\n",
    "    df_mv.to_csv(f, sep=\"\\t\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
